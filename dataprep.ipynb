{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'your/data/directory' # where you store the raw data\n",
    "\n",
    "\"\"\"\n",
    "The raw equity returns data used in this project had daily frequency.\n",
    "One can either keep the daily frequency or go coarser with a weekly frequency\n",
    "by commenting the proper DATA_AGG string.\n",
    "Beware that this affects the frequency of the \"equity input\" described in the thesis\n",
    "and has no effect on the \"financial input\" that has a quarterly frequency\n",
    "\"\"\"\n",
    "# DATA_AGG = 'WEEKLY'\n",
    "DATA_AGG = 'DAILY'\n",
    "\n",
    "# comes from previous iterations of the notebook, should be removed soon\n",
    "OBJECTIVE = 'OPTIMIZATION'\n",
    "\n",
    "# the model optimizes portfolio composed of the top-N marketcap US equities\n",
    "N = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equity prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the weekly aggregation (if requested) otherwise it sorts the data\n",
    "def create_weekly_aggregation_equity_prices(equity_prices):\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        # Create first_day_of_the_week and sort\n",
    "        equity_prices['first_day_of_the_week'] = equity_prices['date'].dt.to_period('W').dt.start_time\n",
    "        equity_prices = equity_prices.sort_values(by=['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # Create weekly aggregated dataframe\n",
    "        equity_prices = equity_prices.groupby(['ticker','first_day_of_the_week']).agg({'closeadj': 'last'}).reset_index(drop=False)\n",
    "        equity_prices = equity_prices.sort_values(by=['first_day_of_the_week'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        equity_prices = equity_prices.sort_values(by=['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    return equity_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes logarithimic equity returns\n",
    "def compute_log_return(equity_prices):\n",
    "    equity_prices['closeadj_lag1'] = equity_prices.groupby(['ticker'])['closeadj'].shift(1)\n",
    "    equity_prices['log_return'] = np.log(equity_prices.closeadj/equity_prices.closeadj_lag1)\n",
    "\n",
    "    return equity_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the Î²-adjusted log returns by subtracting the index (S&P 500) log return which is added with the proper function\n",
    "def add_beta_adjusted(equity_prices, fund_prices):\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        left_date = 'first_day_of_the_week'\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        left_date = 'date'\n",
    "    \n",
    "    # Adding S&P500 returns\n",
    "    equity_prices = equity_prices.sort_values(by=left_date, ascending=True).reset_index(drop=True)\n",
    "    equity_prices['index_log_return'] = pd.merge_asof(left=equity_prices[[left_date,'ticker']], right=fund_prices, on=left_date)['index_log_return']\n",
    "    equity_prices['log_return_beta_adj'] = equity_prices.log_return - equity_prices.index_log_return\n",
    "\n",
    "    return equity_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges data from TTM financials (marketcap, issue date and dividend-adjusted stock price) to the equity returns dataframe\n",
    "def add_market_cap(equity_prices, financials):\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        left_date = 'first_day_of_the_week'\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        left_date = 'date'\n",
    "        \n",
    "    equity_prices.sort_values(by=left_date, inplace=True)\n",
    "    equity_prices.reset_index(drop=True, inplace=True)\n",
    "    financials.sort_values(by='datekey', inplace=True)\n",
    "    financials.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    equity_prices[['marketcap','datekey','price']] = pd.merge_asof(left=equity_prices[[left_date,'ticker']], right=financials[financials.dimension=='ART'],\n",
    "                                        left_on=left_date, right_on='datekey', by='ticker')[['marketcap','datekey','price']]\n",
    "    \n",
    "    return equity_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the ARQ financial dataframe, computes the FCFE and creates the 4 features described in the thesis\n",
    "def add_FCFE(df, financials):\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        left_date = 'first_day_of_the_week'\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        left_date = 'date'\n",
    "\n",
    "    financials['working_capital'] = financials['assetsc'] - financials['liabilitiesc']\n",
    "    financials['working_capital_lag1'] = financials.groupby('ticker')['working_capital'].shift(1)\n",
    "    financials['change_in_working_capital'] = (financials['working_capital'] - financials['working_capital_lag1']).fillna(0)\n",
    "    financials['FCFE'] = financials['netinc'] + financials['depamor'] + financials['capex'] - financials['change_in_working_capital'] + financials['ncfdebt']\n",
    "        \n",
    "    df.sort_values(by=left_date, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    financials.sort_values(by='datekey', inplace=True)\n",
    "    financials.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    financials[['FCFE_lag1','revenue_lag1','ncfo_lag1']] = financials.groupby('ticker')[['FCFE','revenue','ncfo']].shift(1)\n",
    "    financials['change_in_fcfe'] = (financials['FCFE'] - financials['FCFE_lag1']) / (financials['FCFE'].abs())\n",
    "    financials['change_in_revenue'] = (financials['revenue'] - financials['revenue_lag1']) / (financials['revenue'].abs())\n",
    "    financials['change_in_ncfo'] = (financials['ncfo'] - financials['ncfo_lag1']) / (financials['ncfo'].abs())\n",
    "    financials['sign_fcfe'] = financials['FCFE'].apply(np.sign)\n",
    "    \n",
    "    float64_cols = list(financials.select_dtypes(include='float64'))\n",
    "    financials[float64_cols] = financials[float64_cols].astype('float32')\n",
    "\n",
    "    df[features_fin] = pd.merge_asof(left=df[[left_date,'ticker']], right=financials[financials.dimension=='ARQ'][['ticker','datekey']+features_fin],\n",
    "                                     left_on=left_date, right_on='datekey', by='ticker')[features_fin]\n",
    "    print('Added change in FCFE, revenues and NCFO!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates the marketcap with the current dividend-adjusted (close) price\n",
    "def update_marketcap(df):\n",
    "    df['marketcap_updated'] = df['marketcap'] * df['close'] / df['price']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates fut1 entries\n",
    "def create_beta_adj_fut1(equity_prices):\n",
    "    equity_prices['log_return_beta_adj_fut1'] = equity_prices.groupby('ticker')['log_return_beta_adj'].shift(-1)\n",
    "    equity_prices['log_return_fut1'] = equity_prices.groupby('ticker')['log_return'].shift(-1)\n",
    "    equity_prices['index_log_return_fut1'] = equity_prices.groupby('ticker')['index_log_return'].shift(-1)\n",
    "\n",
    "    equity_prices.dropna(subset=['log_return_beta_adj_fut1'], inplace=True)\n",
    "    equity_prices.dropna(subset=['log_return_fut1'], inplace=True)\n",
    "    equity_prices.dropna(subset=['index_log_return_fut1'], inplace=True)\n",
    "    equity_prices.dropna(subset=['marketcap_updated'], inplace=True)\n",
    "    equity_prices.dropna(subset=['change_in_fcfe'], inplace=True)\n",
    "    equity_prices.dropna(subset=['change_in_revenue'], inplace=True)\n",
    "    equity_prices.dropna(subset=['change_in_ncfo'], inplace=True)\n",
    "    equity_prices.dropna(subset=['sign_fcfe'], inplace=True)\n",
    "    equity_prices.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return equity_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fund prices (S&P500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts data for the reference S&P 500 index (^GSPC)\n",
    "def get_fund_prices():\n",
    "    # Open S&P500 data\n",
    "    fund_prices = pd.read_csv(f'{DATA_DIRECTORY}/fund_prices/SHARADAR_SFP_2_fb4f5d2244276f3cfeca03f46b122d99.csv',\n",
    "                                 usecols=['closeadj','ticker','date'], parse_dates=['date'], dtype={'closeadj': 'float32'})\n",
    "    # Sorting\n",
    "    fund_prices = fund_prices.sort_values(by='date', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        # Adds weekly time series and sort accordingly\n",
    "        fund_prices['first_day_of_the_week'] = fund_prices['date'].dt.to_period('W').dt.start_time\n",
    "        date = 'first_day_of_the_week'\n",
    "        lag = 5\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        date = 'date'\n",
    "        lag = 1\n",
    "    \n",
    "    # Create returns\n",
    "    fund_prices[f'closeadj_lag{lag}'] = fund_prices.groupby(['ticker'])['closeadj'].shift(lag)\n",
    "    fund_prices['index_log_return'] = np.log(fund_prices.closeadj/fund_prices[f'closeadj_lag{lag}'])\n",
    "\n",
    "    fund_prices = fund_prices[fund_prices.ticker=='^GSPC']\n",
    "    fund_prices = fund_prices.sort_values(by=date, ascending=True).reset_index(drop=True)\n",
    "\n",
    "    return fund_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering & selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds yearly US bond rates to the dataframe (ended up being unused in the thesis)\n",
    "def is_invalid_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True\n",
    "\n",
    "def add_bond_rates(df):\n",
    "    df.sort_values(by=['date'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    os.chdir(f'{DATA_DIRECTORY}/')\n",
    "    bonds = pd.read_csv('DGS10.csv', sep=';')\n",
    "    bonds.rename(columns={'DATE': 'date'}, inplace=True)\n",
    "    bonds['date'] = pd.to_datetime(bonds['date'], dayfirst=True)\n",
    "    bonds.sort_values(by='date', inplace=True)\n",
    "    bonds.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Clean and prepare data for conversion\n",
    "    bonds['DGS10'] = bonds['DGS10'].str.replace('^\\.$', '', regex=True)  # Remove isolated dots\n",
    "\n",
    "    # Convert to float with error handling\n",
    "    bonds['DGS10'] = pd.to_numeric(bonds['DGS10'], errors='coerce')\n",
    "    bonds.dropna(subset='DGS10', inplace=True)\n",
    "    bonds['DGS10'] = bonds['DGS10'].astype(np.float32)\n",
    "\n",
    "    df['bonds_yearly_rate'] = pd.merge_asof(left=df['date'], right=bonds, on='date')['DGS10']\n",
    "    df['bonds_yearly_rate'] = df['bonds_yearly_rate']/100\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags data with sufficient past & future LAG history to create the matrices later \n",
    "def filter_enough_data(df, LAG, LAG_fin, N_fut):\n",
    "    # Step 1: Sort DataFrame by 'ticker' and 'date'\n",
    "    df = df.sort_values(by=['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Group by 'ticker'\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    valid_list = []\n",
    "\n",
    "    # Process each group separately\n",
    "    for name, group in tqdm(grouped):\n",
    "        df_tick = group.sort_values(by='datekey').reset_index(drop=True)\n",
    "        n = len(group)\n",
    "        \n",
    "        # Create an array to store valid flags for the group\n",
    "        valid_flags = [False] * n\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Calculate number of past entries including current\n",
    "            past_entries = i + 1  # i is 0-based, so i+1 gives the count up to current\n",
    "            \n",
    "            # Calculate number of future entries excluding current\n",
    "            future_entries = n - i - 1  # total length - past - current\n",
    "\n",
    "            # Calculate for financials features\n",
    "            datekey_of_index = df_tick.iloc[i]['datekey']\n",
    "            datekeys = list(df_tick['datekey'].unique())\n",
    "            idx_dk = datekeys.index(datekey_of_index)\n",
    "            \n",
    "            # Check if both conditions are satisfied\n",
    "            if past_entries >= LAG and future_entries >= N_fut and (idx_dk+1 >= LAG_fin):\n",
    "                valid_flags[i] = True\n",
    "        \n",
    "        # Append the results to the valid_list\n",
    "        valid_list.extend(valid_flags)\n",
    "\n",
    "    # Add the result as a new column in the original DataFrame\n",
    "    df['is_data_valid'] = valid_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks each equity by marketcap in each time step\n",
    "def add_market_ranking(df):\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        left_date = 'first_day_of_the_week'\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        left_date = 'date'\n",
    "\n",
    "    df['marketcap_rank_OLD'] = df.groupby(left_date)['marketcap'].rank(ascending=False, method='dense').astype(int)\n",
    "    df['marketcap_rank'] = np.inf\n",
    "\n",
    "    \"\"\"\n",
    "    to avoid ties, we add a small gaussian noise for the ranking only\n",
    "    the values of the noise get printed to ensure that it's indeed small\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    random_noise = np.random.normal(0,1e-3,len(df))\n",
    "    print(f'Random noise has min={random_noise.min()} and max={random_noise.max()}!')\n",
    "    df['marketcap_upd_for_ranking'] = df['marketcap_updated'] + random_noise\n",
    "\n",
    "    df.loc[df['is_data_valid'], 'marketcap_rank'] = df[df['is_data_valid']].groupby(left_date)['marketcap_upd_for_ranking'].rank(ascending=False, method='dense').astype(int)\n",
    "\n",
    "    if (OBJECTIVE == 'OPTIMIZATION'): # & (not do_i_want_sampling)\n",
    "        df['marketcap_sum'] = df[df.marketcap_rank <= N].groupby(left_date)['marketcap'].transform('sum')\n",
    "        df['marketcap_ratio'] = df['marketcap'] / df['marketcap_sum']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM matrix creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Matrices for Portfolio Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "creates the 3D LSTM matrices by handling one ticker at a time and then checking that\n",
    "the shapes are consistent with the expected one (n_samples, history, n_features*N)\n",
    "\"\"\"\n",
    "def create_lstm_matrix_optimization(df_dict, ticker_list, date, features, features_fin, target_val, LAG_history, LAG_history_fin, N_fut,\n",
    "                                    FULL_X, FULL_X_fin, FULL_y, date_flag):\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        df_date = 'first_day_of_the_week'\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        df_date = 'date'\n",
    "\n",
    "    X = []\n",
    "    X_fin = []\n",
    "    y = []\n",
    "\n",
    "    if 'marketcap_ratio' in features:\n",
    "        ind_mcr = features.index('marketcap_ratio')\n",
    "\n",
    "    for ticker in ticker_list:\n",
    "        df = df_dict[ticker].reset_index(drop=True)\n",
    "        idx = df[df[df_date]==date].index[0]\n",
    "\n",
    "        datekey_of_index = df.iloc[idx]['datekey']\n",
    "        datekeys = list(df.sort_values(by='datekey')['datekey'].unique())\n",
    "        idx_dk = datekeys.index(datekey_of_index)\n",
    "\n",
    "        if (idx+1 >= LAG_history) and (idx < (len(df) - N_fut)) and (idx_dk+1 >= LAG_history_fin):\n",
    "            # Append past `LAG_history` features to `X`\n",
    "            X.append(df[features].iloc[idx - LAG_history + 1:idx + 1].values.reshape((1, LAG_history, len(features))))\n",
    "            X_fin.append(df.loc[df.datekey.isin(datekeys[idx_dk - LAG_history_fin + 1:idx_dk + 1]), features_fin+['datekey']].drop_duplicates()[features_fin].values.reshape((1, LAG_history_fin, len(features_fin))))\n",
    "            \n",
    "            # Append future `N_fut` target values to `y`\n",
    "            # Indices are ok if target_val is NOT fut1\n",
    "            y.append(df[target_val].iloc[idx + 1:idx + N_fut + 1].values.reshape((1, N_fut, len(target_val))))\n",
    "\n",
    "    if len(X) > 0:\n",
    "        X = np.concatenate(X, axis=2)\n",
    "        X = X.astype(np.float32)\n",
    "    else:\n",
    "        X = np.asarray(X)\n",
    "    if len(y) > 0:\n",
    "        y = np.concatenate(y, axis=2)\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = np.asarray(y)\n",
    "    if len(X_fin) > 0:\n",
    "        X_fin = np.concatenate(X_fin, axis=2)\n",
    "        X_fin = X_fin.astype(np.float32)\n",
    "    else:\n",
    "        X_fin = np.asarray(X_fin)\n",
    "    \n",
    "    if (X.shape == (1, LAG_history, len(features)*len(ticker_list))) & (y.shape == (1, N_fut, len(target_val)*len(ticker_list))) & (X_fin.shape == (1, LAG_history_fin, len(features_fin)*len(ticker_list))):\n",
    "        # If it's not the correct shape it means that there is some missing data thus we ignore the entry\n",
    "        if 'marketcap_ratio' in features: # The past history will have the same marketcap_ratio of the considered date\n",
    "            X[:,:,ind_mcr::len(features)] = X[:,-1,ind_mcr::len(features)] \n",
    "            \n",
    "        date_flag.append(date)\n",
    "        if len(FULL_X) == 0 & len(FULL_y) == 0:\n",
    "            FULL_X = np.copy(X)\n",
    "            FULL_y = np.copy(y)\n",
    "            FULL_X_fin = np.copy(X_fin)\n",
    "        else:\n",
    "            FULL_X = np.concatenate([FULL_X, X], axis=0)\n",
    "            FULL_y = np.concatenate([FULL_y, y], axis=0)\n",
    "            FULL_X_fin = np.concatenate([FULL_X_fin, X_fin], axis=0)\n",
    "\n",
    "    return FULL_X, FULL_X_fin, FULL_y, date_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the X & y matrices and the support dataframe\n",
    "def save_matrix(X, X_fin, y, support, sampling=False, date_index=0):\n",
    "    if OBJECTIVE == 'OPTIMIZATION':\n",
    "        directory = f'{DATA_DIRECTORY}/LSTM MATRICES/{DATA_AGG}/{OBJECTIVE}/LAG HISTORY {LAG_history} & LAG FIN {LAG_history_fin} & N FUTURES {N_futures}/FEATURES_{features}+{features_fin} - TARGET_{target_val}/TOP-{N} MARKETCAP'\n",
    "        os.makedirs(directory,exist_ok=True)\n",
    "        os.chdir(directory)\n",
    "        np.save(f'TOP-{N}_matrix_X_{DATA_AGG}', X)\n",
    "        np.save(f'TOP-{N}_matrix_X_fin_{DATA_AGG}', X_fin)\n",
    "        np.save(f'TOP-{N}_matrix_y_{DATA_AGG}', y)\n",
    "        support.to_csv(f'TOP-{N}_support_{DATA_AGG}.csv', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing to get matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "filters the initial dataframe to consider only tickers that are in the top-N marketcap\n",
    "then creates matrices and support dataframe for each timee step (not a single one due to memory constraints)\n",
    "\"\"\"\n",
    "def process_for_optimization(df, feature, features_fin, target, LAG, LAG_fin, N_fut, start_date='1800-01-01'):\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        df_date = 'first_day_of_the_week'\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        df_date = 'date'\n",
    "\n",
    "    if OBJECTIVE == 'OPTIMIZATION':\n",
    "        topN_ticker_list = df.loc[df.marketcap_rank <= N, 'ticker'].unique()\n",
    "        df = df[df['ticker'].isin(topN_ticker_list)].reset_index(drop=True)\n",
    "\n",
    "        df = df.sort_values(by=[df_date,'marketcap_rank']).reset_index(drop=True)\n",
    "        dates_list = df.loc[df[df_date]>=start_date, df_date].unique()\n",
    "\n",
    "        df_dict = {k: v for k,v in df.groupby('ticker')}\n",
    "\n",
    "        X = []\n",
    "        X_fin = []\n",
    "        y = []\n",
    "        used_dates = []\n",
    "\n",
    "        for date in tqdm(dates_list):\n",
    "            tickers_list = df.loc[(df.marketcap_rank <= N) & (df[df_date]==date), 'ticker'].unique()\n",
    "            X, X_fin, y, used_dates = create_lstm_matrix_optimization(df_dict, tickers_list, date, feature, features_fin, target, LAG, LAG_fin, N_fut,\n",
    "                                                                      X, X_fin, y, used_dates)\n",
    "        support = df[(df.marketcap_rank <= N) & (df[df_date].isin(used_dates))]\n",
    "\n",
    "        save_matrix(X,X_fin,y,support)\n",
    "    else:\n",
    "        print('This function is for OPTIMIZATION only!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "select which features to use:\n",
    "    - features come from data with daily (or weekly) frequency [equity_prices, insiders]\n",
    "    - features_fin come from data with quarterly frequency [financials]\n",
    "    - target_val will define the feature contained in the y matrix\n",
    "\"\"\"\n",
    "\n",
    "features = ['log_return_beta_adj', 'marketcap_ratio']\n",
    "features_fin = ['change_in_fcfe','change_in_revenue','change_in_ncfo','sign_fcfe']\n",
    "\n",
    "target_val = ['log_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_history = 30 # time series history for features\n",
    "LAG_history_fin = 8 # time series history for features_fin\n",
    "N_futures = 10 # time series future history for features_fin\n",
    "\n",
    "df_name = 'your_merged_dataframe_name.csv' # name of the dataframe that contains all the processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads equity prices datafram\n",
    "lstm_df = pd.read_csv(f'{DATA_DIRECTORY}/equity_prices/SHARADAR_SEP_2_0afbc06bfa7d2d5ebd28c43e0940ec30.csv',\n",
    "                               usecols=['date','ticker','closeadj','close'], dtype={'closeadj': 'float32', 'close': 'float32'}, parse_dates=['date'])\n",
    "print('Loaded equity_prices!')\n",
    "\n",
    "# loads financials dataframe\n",
    "financials_df = pd.read_csv(f'{DATA_DIRECTORY}/financials/SHARADAR_SF1_2_1ef2651587ad65788e4bc47f7728edfe.csv',\n",
    "                            usecols=['datekey','dimension','ticker','shareswa','marketcap','price','netinc','depamor','capex',\n",
    "                                     'assetsc','liabilitiesc','ncfo','revenue', 'debtc', 'ncfdebt'], parse_dates=['datekey'])\n",
    "print('Loaded financials!')\n",
    "\n",
    "# loads the fund prices dataframe\n",
    "fund_prices_df = get_fund_prices()\n",
    "print('Loaded fund prices!')\n",
    "\n",
    "# get minimum and maximum dates (only used for WEEKLY aggregation)\n",
    "def first_day_of_week(date):\n",
    "    # Assuming week starts on Monday\n",
    "    return date - datetime.timedelta(days=date.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the equity prices, financials and fund prices dataframe to generated the final processed dataframe\n",
    "lstm_df = create_weekly_aggregation_equity_prices(lstm_df)\n",
    "lstm_df = compute_log_return(lstm_df)\n",
    "lstm_df = add_beta_adjusted(lstm_df, fund_prices_df)\n",
    "del fund_prices_df\n",
    "lstm_df = add_market_cap(lstm_df, financials_df)\n",
    "lstm_df = add_FCFE(lstm_df, financials_df)\n",
    "del financials_df\n",
    "lstm_df = update_marketcap(lstm_df)\n",
    "print('Updated marketcap!')\n",
    "lstm_df = create_beta_adj_fut1(lstm_df)\n",
    "print('Finished processing equity prices!')\n",
    "\n",
    "float64_cols = list(lstm_df.select_dtypes(include='float64'))\n",
    "lstm_df[float64_cols] = lstm_df[float64_cols].astype('float32')\n",
    "lstm_df.sort_values(by='date', inplace=True)\n",
    "lstm_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "if 'marketcap_ratio' in features:\n",
    "    assert features.index('marketcap_ratio') == len(features)-1\n",
    "    fine_features_droppabili = len(features)-1\n",
    "else:\n",
    "    fine_features_droppabili = len(features)\n",
    "\n",
    "lstm_df = add_bond_rates(lstm_df)\n",
    "print('Added bonds!')\n",
    "\n",
    "lstm_df[target_val+features[:fine_features_droppabili]+['bonds_yearly_rate']] = lstm_df[target_val+features[:fine_features_droppabili]+['bonds_yearly_rate']].replace([np.inf, -np.inf], np.nan)\n",
    "lstm_df[features_fin] = lstm_df[features_fin].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print('Dropping NaNs!')\n",
    "lstm_df.dropna(subset=target_val+features[:fine_features_droppabili], inplace=True)\n",
    "lstm_df.dropna(subset=['bonds_yearly_rate'], inplace=True)\n",
    "lstm_df.dropna(subset=features_fin, inplace=True)\n",
    "lstm_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "lstm_df = filter_enough_data(lstm_df, LAG_history, LAG_history_fin, N_futures)\n",
    "print('Filtered!')\n",
    "lstm_df = add_market_ranking(lstm_df)\n",
    "print('Added marketcap ranking and marketcap_ratio!')\n",
    "lstm_df.loc[:, 'marketcap_updated'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "lstm_df.dropna(subset=['marketcap_updated'], inplace=True)\n",
    "\n",
    "lstm_df.reset_index(drop=True, inplace=True)\n",
    "lstm_df.drop(columns=['close','closeadj','closeadj_lag1','price'], inplace=True)\n",
    "print('Finished processing dataframe for LSTM!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save DataFrame for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the processed dataframe\n",
    "os.chdir(f'{DATA_DIRECTORY}/LSTM MATRICES/{DATA_AGG}/{OBJECTIVE}')\n",
    "lstm_df.to_csv(df_name, mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading LSTM DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:23, 16.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# loads the dataframe in float32\n",
    "os.chdir(f'{DATA_DIRECTORY}/LSTM MATRICES/{DATA_AGG}/{OBJECTIVE}')\n",
    "\n",
    "if 'marketcap_ratio' in features:\n",
    "    assert features.index('marketcap_ratio') == len(features)-1\n",
    "    feat_stop = len(features)-1\n",
    "else:\n",
    "    feat_stop = len(features)\n",
    "\n",
    "datatypes_for_df = {'marketcap': 'float32', 'marketcap_rank': 'float32', 'index_log_return_fut1': 'float32', 'bonds_yearly_rate': 'float32',\n",
    "                    'index_log_return': 'float32', 'marketcap_updated': 'float32', 'marketcap_ratio': 'float32'}\n",
    "datatypes_for_df.update({feature: 'float32' for feature in features})\n",
    "datatypes_for_df.update({feature+'_fut1': 'float32' for feature in features[:feat_stop]})\n",
    "datatypes_for_df.update({feature_fin: 'float32' for feature_fin in features_fin})\n",
    "datatypes_for_df.update({target: 'float32' for target in target_val})\n",
    "\n",
    "if OBJECTIVE == 'OPTIMIZATION':\n",
    "    if DATA_AGG == 'WEEKLY':\n",
    "        df_date = 'first_day_of_the_week'\n",
    "        lstm_df = pd.read_csv(df_name, parse_dates=[df_date],\n",
    "                        usecols=([df_date, 'ticker', 'index_log_return','marketcap', 'marketcap_rank', 'bonds_yearly_rate', 'index_log_return_fut1', 'datekey',\n",
    "                                 'relative_shares', 'change_in_portfolio', 'closeadj', 'marketcap_updated', 'datekey'] +\n",
    "                                 features + [feature+'_fut1' for feature in features[:feat_stop]] + [feature_fin for feature_fin in features_fin] +\n",
    "                                 [target for target in target_val]),\n",
    "                        dtype=datatypes_for_df)\n",
    "    elif DATA_AGG == 'DAILY':\n",
    "        # loading by chunks because of memory contraints\n",
    "        df_date = 'date'\n",
    "        lstm_df = []\n",
    "\n",
    "        for chunk in tqdm(pd.read_csv(df_name, parse_dates=[df_date],\n",
    "                        usecols=([df_date, 'ticker', 'index_log_return', 'marketcap', 'marketcap_rank', 'bonds_yearly_rate', 'index_log_return_fut1',\n",
    "                                  'marketcap_ratio', 'marketcap_updated', 'datekey'] + features + [feature+'_fut1' for feature in features[:feat_stop]] +\n",
    "                                  [feature_fin for feature_fin in features_fin] + [target for target in target_val]),\n",
    "                        dtype=datatypes_for_df, chunksize=5000000)):\n",
    "            lstm_df.append(chunk)\n",
    "        lstm_df = pd.concat(lstm_df)\n",
    "        lstm_df['datekey'] = pd.to_datetime(lstm_df['datekey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing by chunks (because of memory issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting creation of top-500 matrices for optimization (without sampling)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3571/3571 [1:03:07<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# starts the pipeline to create the single timestep X & y matrices and the support dataframes\n",
    "if OBJECTIVE == 'OPTIMIZATION':\n",
    "    print(f'Starting creation of top-{N} matrices for optimization (without sampling)!')\n",
    "    process_for_optimization(lstm_df, features, features_fin, target_val, LAG_history, LAG_history_fin, N_futures, start_date='2010-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving S&P500 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the index data\n",
    "os.chdir(f'{DATA_DIRECTORY}/LSTM MATRICES/{DATA_AGG}')\n",
    "sp500 = get_fund_prices()\n",
    "sp500['index_log_return_fut1'] = sp500['index_log_return'].shift(-1)\n",
    "sp500.dropna(subset=['index_log_return_fut1'], inplace=True)\n",
    "sp500.reset_index(drop=True, inplace=True)\n",
    "sp500.to_csv('S&P500_index_return_DAILY.csv', mode='w', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
